{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51de2e2e-b482-47c0-98eb-ca5d4c90451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import Transformer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ae2c30c-5114-462a-b409-5c044f4887be",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file = \"Dataset eng-kanada\\english.txt\"\n",
    "kannada_file = \"Dataset eng-kanada\\kannada.txt\"\n",
    "\n",
    "# generate the filtering appendix code\n",
    "\n",
    "START_TOKEN = 'sitati'\n",
    "PADDING_TOKEN = 'pido'\n",
    "END_TOKEN = 'endas'\n",
    "\n",
    "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
    "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
    "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n",
    "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n",
    "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n",
    "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n",
    "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n",
    "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n",
    "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n",
    "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n",
    "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@',\n",
    "                        '[', ']', '^', '_', '\\\\', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9f7f9c0-b8b6-4322-b5a5-db8b3013a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
    "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56244d3e-e5a9-4d2d-a7e8-3c80a8a575cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_file, 'r', encoding='utf-8') as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(kannada_file, 'r', encoding='utf-8') as file:\n",
    "    kannada_sentences = file.readlines()\n",
    "\n",
    "# Limit Number of sentences\n",
    "TOTAL_SENTENCES = 100000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
    "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
    "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f0803440-8701-44b0-a621-a699afceddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hes a scientist.',\n",
       " \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
       " '8 lakh crore have been looted.',\n",
       " 'i read a lot into this as well.',\n",
       " \"she was found dead with the phone's battery exploded close to her head the following morning.\",\n",
       " 'how did mankind come under satans rival sovereignty?',\n",
       " 'and then i became prime minister.',\n",
       " 'what about corruption?',\n",
       " 'no differences',\n",
       " '\"\"\"the shooting of the film is 90 percent done.\"']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6e24bb42-b37a-409d-8eca-e21d9c8d6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
       " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
       " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
       " 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
       " 'ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್\\u200cನ ಬ್ಯಾಟರಿ ಸ್ಫೋಟಗೊಂಡು ಆಕೆ ಮೃತಪಟ್ಟಿದ್ದಾಳೆ ಎನ್ನಲಾಗಿದೆ.',\n",
       " 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?',\n",
       " 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.',\n",
       " 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?',\n",
       " '‘ಅನುಪಾತದಲ್ಲಿ ವ್ಯತ್ಯಾಸವಿಲ್ಲ’',\n",
       " 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kannada_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "085a8125-723a-4afa-b86a-9a18a806b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length Kannada: 172.0\n",
      "97th percentile length English: 179.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01714bf2-f9b2-4c9b-a454-7a2a11163c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 100000\n",
      "Number of valid sentences: 81916\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 200\n",
    "\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(kannada_sentences)):\n",
    "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
    "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af9563f8-ed80-46e6-bbf7-8bd5725b56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "adf0bd8d-a32b-424b-a247-011c30a3ec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
       " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
       " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kannada_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88eb225f-d7f6-4574-b07a-aeac546eb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 200\n",
    "kn_vocab_size = len(kannada_vocabulary)\n",
    "\n",
    "transformer = Transformer(d_model, \n",
    "                          ffn_hidden,\n",
    "                          num_heads, \n",
    "                          drop_prob, \n",
    "                          num_layers, \n",
    "                          max_sequence_length,\n",
    "                          kn_vocab_size,\n",
    "                          english_to_index,\n",
    "                          kannada_to_index,\n",
    "                          START_TOKEN, \n",
    "                          END_TOKEN, \n",
    "                          PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5eb6113a-823f-4fc5-a4a7-de5ffb9d1b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(70, 512)\n",
       "      (position_encoder): PositionEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequantialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalization()\n",
       "        (drop2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(125, 512)\n",
       "      (position_encoder): PositionEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNormalization()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=125, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a6410b6-da05-4204-a59a-69bc4238dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, kannada_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.kannada_sentences = kannada_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.kannada_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4997749e-3a91-47bd-9c1d-ec095c089ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(english_sentences, kannada_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1832cc9f-5d94-40ab-a3e4-0e02c1aa4f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81916"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c3cafc3b-4813-404a-9012-0836e8c1d2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
       " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6df19dde-0f06-4817-b8c5-7899b72f9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ff5d97e3-6579-4f9e-9f66-fdb2b80e3f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hes a scientist.', \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\", '8 lakh crore have been looted.', 'i read a lot into this as well.', 'how did mankind come under satans rival sovereignty?', 'and then i became prime minister.', 'what about corruption?', '\"\"\"the shooting of the film is 90 percent done.\"', 'the special statute', '\"then the king said to ittai the gittite, \"\"why do you also go with us? return, and stay with the king. for you are a foreigner, and also an exile. return to your own place.\"', 'what happened at the un general assembly?', 'the meeting was attended by prime minister narendra modi, home minister amit shah and defence minister rajnath singh, among others.', 'it has been under discussion for a long time.', 'buses cannot get there.', 'why then this tradition was not thought of?', 'kashmiri youth join indian army', 'basic amenities elude this village', 'off-budget borrowings of the state increased from rs853 crore in 2011-12 to rs,173 crore in 2017-18', 'during the monsoon season, the rubbish is swept on to the road by rainwater, creating problems for the traffic.', 'however, the other two, ramesh jarkiholi and mahesh kumatahalli, had not given any reason, he said.', 'how to make pasta salad?', 'he accused the modi government of ruining the countrys economy.', 'add chopped carrots and potatoes.', 'first shot', 'after the incident, police reached the spot and admitted the injured to the hospital.', 'her father gave kunti to his childless cousin kuntibhoja.', 'how to eat', 'granted, the standard of living varies from country to country.', 'for example, medical.', 'thats a fine plan.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.', 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.', 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು', 'ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.', 'ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?', 'ಭಾರತೀಯ ಸೇನೆ ಸೇರಲು ಮುಗಿಬೀಳುತ್ತಿರುವ ಕಾಶ್ಮೀರಿ ಯುವಕರು !', 'ಕುಗ್ರಾಮವಾದ ಈ ಹಳ್ಳಿಯಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳು', 'ರಾಜ್ಯದ ಬಜೆಟ್ ಯೇತರ ಸಾಲವು 2011-12ರಲ್ಲಿದ್ದ 1,853ಕೋಟಿ ರೂಪಾಯಿಯಿಂದ 2017-18ರಲ್ಲಿ 13,173 ಕ್ಕೆ ಏರಿಕಯಾಗಿದೆ.', 'ಮಳೆಗಾಲದಲ್ಲಿ ಕೊಳಚೆ ನೀರಿನೊಂದಿಗೆ ಮಳೆನೀರು ರಸ್ತೆಯಲ್ಲಿಯೇ ಮಡುಗಟ್ಟಿ ನಿಂತು ಸೊಳ್ಳೆಗಳ ಹೆಚ್ಚಳಕ್ಕೆ ಕಾರಣವಾಗುತ್ತಿದೆ.', 'ಯಾವುದೇ ಕಾರಣ ನೀಡದೆ ರಮೇಶ್ ಜಾರಕಿಹೊಳಿ ಮತ್ತು ಮಹೇಶ್ ಕಮಟಹಳ್ಳಿ ಗೈರು ಹಾಜರಾಗಿದ್ದಾರೆ ಎಂದು ಮಾಹಿತಿ ನೀಡಿದ್ದಾರೆ.', '\"ಹೇಗೆ \"\"ಗೂಳಿಕಾಳಗ\"\" ಒಂದು ಸಲಾಡ್ ತಯಾರು ಹೇಗೆ?\"', 'ಅದ್ರಲ್ಲೂ ಪ್ರಮುಖವಾಗಿ ದೇಶದ ಆರ್ಥಿಕತೆ ಪಾತಾಳಕ್ಕೆ ಕುಸಿದಿರೋದಕ್ಕೆ ಮೋದಿ ಸರ್ಕಾರವೇ ಸರ್ಕಾರ ಅಂತಾ ಟೀಕಿಸುತ್ತಿದ್ದಾರೆ.', 'ಅವರಿಗೆ ಕತ್ತರಿಸಿದ ಪಾರ್ಸ್ಲಿ ಮತ್ತು ತುಳಸಿ ಸೇರಿಸಿ.', 'ಮೊದಲ ಚಿತ್ರ ಶೂಟಿಂಗ್', 'ಅಪಘಾತದ ನಡೆದ ಕೂಡಲೇ ಮಾಹಿತಿ ಪಡೆದು ಸ್ಥಳಕ್ಕೆ ಆಗಮಿಸಿದ ಪೋಲಿಸರು ಗಾಯಗೊಂಡವರನ್ನು ಆಸ್ಪತ್ರೆ ದಾಖಲಿಸಿದ್ದಾರೆ.', 'ಆಕೆಯ ತಂದೆ ತನ್ನ ಮಕ್ಕಳಿಲ್ಲದ ಸೋದರಸಂಬಂಧಿ ಕುಂತಿಭೋಜನಿಗೆ ಕುಂತಿಯನ್ನು ಕೊಟ್ಟರು.', 'ಆಹಾರ ಬೇಯಿಸುವುದು ಹೇಗೆ', 'ಜೀವನಮಟ್ಟವು ದೇಶದಿಂದ ದೇಶಕ್ಕೆ ಬದಲಾಗುತ್ತದೆ ನಿಜ.', 'ಉದಾಹರಣೆಗೆ, ಖಗೋಳಶಾಸ್ತ್ರ.', 'ಇದೊಂದು ಉತ್ತಮ ಯೋಜನೆ.')]\n",
      "[(\"you don't know this.\", 'you are respected in society.', 'is this pic real?', '\"\"\"felicitations to all for the foundation laying of ram temple in ayodhya.\"', 'she looked stunningly beautiful in that', '{ -brand-name-nightly } blog', 'it was consistent.', 'case has been registered in malpe police station.', 'they still exist to this day.', 'breaking up isnt easy', 'students should abide by the rules.', 'we try to understand her well.', 'then the angel of yahweh commanded gad to tell david that david should go up, and raise an altar to yahweh in the threshing floor of ornan the jebusite.', 'for example, he forbids sexual immorality, idolatry, stealing, and drunkenness.', 'it is also a famous tourist destination', 'doctors and other persons concerned have been questioned.', 'but in politics.', 'to understand gods actions, we need to answer three questions: (1) who starts the war?', '14,000 crores.', 'what are those words?', 'shivakumars assumption of office as president of karnataka pradesh congress committee (kpcc).', 'now we were notified that we would receive the magazines in russian only.', 'they are found along coastlines around the world except antarcticas.', 'bill clinton went on to become president.', 'the culmination of the three stories towards the end. gives a new dimension to the film as a whole and concludes shuddhi.', 'while individuals may be allowed to die, god will never allow the extermination of his people as a whole.', '(the author is an educationist)', 'therefore i will not refrain my mouth. i will speak in the anguish of my spirit. i will complain in the bitterness of my soul.', 'are they hungry for knowledge?', 'the door did not open.'), ('ಈ ಬಗ್ಗೆ ನೀವಿನ್ನು ತಿಳಿದಿಲ್ಲ ಎ .', 'ಸಮಾಜದಲ್ಲಿ ಗೌರವವಿರುತ್ತದೆ.', 'ಈ ಫೋಟೋ ವಾಸ್ತವೋ, ಅಸಲಿಯೋ ?', \"'ಅಯೋಧ್ಯೆಯ ರಾಮ ದೇವಾಲಯದ ಅಡಿಪಾಯ ಹಾಕಿದ್ದಕ್ಕಾಗಿ ಎಲ್ಲರಿಗೂ ಶುಭಾಶಯಗಳು.\", 'ಅವಳು ಅಥವಾ ಆತ ನೋಡಲು ತುಂಬಾ ಸುಂದರ', 'ನೈಟ್ಲಿ ಬ್ಲಾಗ್', 'ಇದು ಸಂಕೇತವಾಗಿತ್ತು.', 'ಪ್ರಕರಣ ಮಳವಳ್ಳಿ ಪೊಲೀಸ್ ಠಾಣೆಯಲ್ಲಿ ದಾಖಲಾಗಿದೆ.', 'ಅಂತೆಯೇ ಅವರು ಇಂದಿಗೂ ಪ್ರಸ್ತುತವಾಗಿದ್ದಾರೆ.', 'ವೈರಾಗ್ಯ ಸುಲಭವಲ್ಲ', 'ವಿದ್ಯಾರ್ಥಿಗಳು ಕೂಡಾ ಶಿಸ್ತು ಪಾಲಿಸಬೇಕು.', '\"\"\" ಅವರೊಂದಿಗೆ ಅವನಿಗೆ ವಿವರವಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಪ್ರಯತ್ನಿಸೋಣ.\"', 'ಆಗ ಕರ್ತನ ದೂತನು ಗಾದನಿಗೆ--ದಾವೀದನು ಹೋಗಿ ಯೆಬೂಸಿಯನಾದ ಒರ್ನಾನನ ಕಣದಲ್ಲಿ ಕರ್ತನಿಗೆ ಬಲಿಪೀಠವನ್ನು ಕಟ್ಟುವ ಹಾಗೆ ದಾವೀದನಿಗೆ ಹೇಳು ಅಂದನು.', 'ಬೈಬಲಿನಲ್ಲಿ ಯೆಹೋವನು ನಮಗೆ ಸ್ಪಷ್ಟ ಆಜ್ಞೆಗಳನ್ನು ಕೊಟ್ಟಿದ್ದಾನೆ.', 'ಇದೊಂದು ಬಹು ಜನಪ್ರಿಯವಾದ ವಿಹಾರ ತಾಣವಾಗಿದೆ', 'ಶಸ್ತ್ರ ಚಿಕಿತ್ಸೆ ಮಾಡಿದ ವೈದ್ಯರು ಮತ್ತು ಇತರ ಸಿಬ್ಬಂದಿಯನ್ನು ವಿಚಾರಣೆಗೆ ಒಳಪಡಿಸಿದ್ದಾರೆ.', 'ಆದರೆ ರಾಜಕೀಯದಲ್ಲಿರುತ್ತೇನೆ.', '( ನೆಹೆಮಿಾಯ 9: 17) ಇದನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಈ ಮೂರು ಪ್ರಶ್ನೆಗಳಿಗೆ ಉತ್ತರ ತಿಳಿಯಿರಿ: (1) ಯುದ್ಧ ಆರಂಭಿಸುವವರು ಯಾರು?', '14,000 ಒಟ್ಟುಗೂಡಿತು.', 'ಈ ಶಬ್ದಗಳು ಯಾವುವು?', 'ಕೊನೆಗೂ ಡಿಕೆ ಶಿವಕುಮಾರ್ ಅವರನ್ನ ಕರ್ನಾಟಕ ಪ್ರದೇಶ ಕಾಂಗ್ರೆಸ್ (ಕೆಪಿಸಿಸಿ) ಅಧ್ಯಕ್ಷರನ್ನಾಗಿ ಆಯ್ಮೆ ಮಾಡಿದೆ.', 'ಪತ್ರಿಕೆಯು ನಿಷೇಧಿಸಲ್ಪಟ್ಟಿತ್ತು, ಆದರೆ ಇತರ ಸ್ಥಳಗಳಿಂದ ಗುಟ್ಟಾಗಿ ನಾವು ಪತ್ರಿಕೆಗಳನ್ನು ಪಡೆಯುತ್ತಿದ್ದೆವು.', 'ಅಂಟಾರ್ಟಿಕಾ ಖಂಡದ ಹೊರತುಪಡಿಸಿ ಅವು ವಿಶ್ವದಾದ್ಯಂತ ಕಂಡುಬರುತ್ತವೆ.', 'ಇವರ ನಂತರ ಬಿಲ್ ಕ್ಲಿಂಟನ್ ಅಧ್ಯಕ್ಷ ಪಟ್ಟ ಅಲಂಕರಿಸಿದ್ದರು.', 'ಕೊನೆಯಲ್ಲಿ ಮೂರು ಕಥೆಗಳ ಪರಾಕಾಷ್ಠೆ. ಒಟ್ಟಾರೆಯಾಗಿ ಚಿತ್ರಕ್ಕೆ ಹೊಸ ಆಯಾಮವನ್ನು ನೀಡುತ್ತದೆ ಮತ್ತು ಸುಧಿಯನ್ನು ಮುಕ್ತಾಯಗೊಳಿಸುತ್ತದೆ.', 'ಕೆಲವು ವ್ಯಕ್ತಿಗಳು ಸಾಯುವಂತೆ ಅನುಮತಿಸಲ್ಪಡುವುದಾದರೂ, ತನ್ನ ಜನರೆಲ್ಲರೂ ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಮೂಲರಾಗುವಂತೆ ದೇವರು ಎಂದಿಗೂ ಅನುಮತಿಸುವುದಿಲ್ಲ.', '(ಲೇಖಕರು ಶಿಕ್ಷಣತಜ್ಞರು)', 'ಆದದರಿಂದ ನಾನು ನನ್ನ ಬಾಯಿಯನ್ನು ಮುಚ್ಚು ವದಿಲ್ಲ. ನಾನು ಆತ್ಮ ವೇದನೆಯಿಂದ ಮಾತನಾಡು ವೆನು. ನನ್ನ ಮನೋವ್ಯಥೆಯಲ್ಲಿ ನಾನು ಗುಣುಗುಟ್ಟು ವೆನು.', 'ನಿಂಗೆ ಜ್ಞಾನದ ಹಸಿವು ನೀಗಸ್ತಾರೆ ?', 'ಅಂಗಳದ ಬಾಗಿಲು ಹಾಕಿರಲಿಲ್ಲ.')]\n",
      "[('i am non-vegetarian.', 'most of the government schools lack the teachers.', 'to the fans.', 'during the investigation, it was found that the 130 activists were in regular contact with the jmb leadership, he said.', 'it is 260 km away from mumbai city.', 'not everything happens as we expect.', 'what are you talking?', 'that young couple certainly started their marriage off on the best of foundations.', 'abbey waterfall flows from a height of 60 feet.', 'the treatment depends largely on the type and stage of the cancer.', 'i learned how to cook.', 'is coconut oil harmful?', 'may i answer?', 'what is love?', 'symptoms include rashes, diarrhea, vomiting, stomach cramps and bloating.', 'during 2015, they plan on introducing two new amg vehicles the c 63 s and gt s supercar', 'candidates belonging to st/sc, pwd category are exempted from paying any fee.', 'i love', 'you cant have everything always.', 'the death toll of coronavirus rose to 718 as 37 casualties were reported in 24 hours', 'his life was devoted to the service of the country.', 'he lost his father at an early age.', 'the vehicle is completely burnt.', 'but it was cancelled at the last moment.', 'many people are dead due to floods and extensive damage to property has occurred.', 'the scheme will be rolled out in the next couple of months.', 'what is not nice?', 'and he said, bring me a new cruse, and put salt therein. and they brought it to him.', 'the tvs entorq 125 will be up against the likes of honda activa, suzuki access and others', 'he handed over the documents for the 900 sq ft plot to nagar panchayat chairman zahir farooqui.'), ('ನಾನೇನು ಸಂಪೂರ್ಣ ಸಸ್ಯಾಹಾರಿ ಅಲ್ಲ.', 'ಬಹುತೇಕ ಸರ್ಕಾರಿ ಶಾಲೆಗಳಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳೇ ಇಲ್ಲದಿರುವುದು ಕಂಡುಬಂದಿದೆ.', 'ಎಂಬ ಅಭಿಮಾನಿಗಳಿಗೆ ಕಾಡುತ್ತಿದೆ.', 'ಜೆಎಂಬಿ ನಾಯಕತ್ವದ ಜತೆ 130 ಕಾರ್ಯಕರ್ತರು ನಿಕಟ ಸಂಪರ್ಕದಲ್ಲಿರುವುದು ತನಿಖಾ ಹಂತದಲ್ಲಿ ಬೆಳಕಿಗೆ ಬಂದಿದೆ ಎಂದು ತಿಳಿಸಿದರು.', 'ಇದು ರಾಜ್ಯದ ರಾಜಧಾನಿಯಾದ ಮುಂಬೈನಿಂದ ಸುಮಾರು 260 ಕಿಮೀ ದೂರದಲ್ಲಿದೆ.', 'ಎಲ್ಲವೂ ನಾವು ಅಂದುಕೊಂಡಂತೆಯೇ ಆಗುವುದಿಲ್ಲ.', '\"\"\"ಏನೆಲ್ಲ ಮಾತಾಡತಾನೆ?\"', 'ಕೊನೆಯದಾಗಿ, ಈ ಘಟನೆಗಳು ಯೋಸೇಫ ಮರಿಯ ಇಬ್ಬರಿಗೂ ಪ್ರಾಮಾಣಿಕವೂ ಮುಕ್ತವೂ ಆದ ಸಂವಾದವು ಎಷ್ಟು ಮಹತ್ವ ಎಂಬ ವಿಷಯದಲ್ಲಿ ಹೆಚ್ಚನ್ನು ಕಲಿಸಿದ್ದಿರಬೇಕು.', '60 ಅಡಿ ಎತ್ತರದಿಂದ ನೀರು ಧರೆಗೆ ಧುಮ್ಮಿಕ್ಕುತ್ತದೆ.', 'ಚಿಕಿತ್ಸೆಯ ವಿಧಾನದ ಆಯ್ಕೆ ಹೆಚ್ಚಾಗಿ ರೋಗದ ಹಂತ ಮತ್ತು ಸ್ವರೂಪವನ್ನು ಅವಲಂಬಿಸಿರುತ್ತದೆ.', 'ಅಷ್ಟುಇಷ್ಟು ಅಡುಗೆ ಮಾಡುವುದನ್ನು ಕಲಿತುಬಿಟ್ಟೆ.', 'ಕಾರ್ನ್ ಎಣ್ಣೆ ಉಪಯುಕ್ತವಾಗಿದೆ?', '\"ಪ್ರಶ್ನೆ, \"\"ನಾನು ಉತ್ತರಿಸಬೇಕೇ?\"', 'ಪ್ರೀತಿ ಅಂದರೆ ಇದೇನಾ?', 'ರೋಗಲಕ್ಷಣಗಳು ಕ್ಯಾಂಪಿಂಗ್, ಭಾರಿ ಅವಧಿ, ಹೆಪ್ಪುಗಟ್ಟುವಿಕೆ, ಕೆಳ ಹೊಟ್ಟೆ ನೋವು, ಮತ್ತು ಉಬ್ಬುವುದು.', 'ಈ ಸಾಲಿಗೆ ಪ್ರಸಕ್ತ ವರ್ಷದಲ್ಲಿ ಎಎಂಜಿ ಸಿ 63 ಎಸ್ ಮತ್ತು ಜಿಟಿ ಎಸ್ ಸೂಪರ್ ಕಾರು ಸೇರ್ಪಡೆಯಾಗಲಿದೆ', 'ಎಸ್ಸಿ, ಎಸ್ಟಿ / ಪಿಡಬ್ಲ್ಯೂಡಿ ವಿಭಾಗಗಳಿಗೆ ಸೇರಿದ ಅಭ್ಯರ್ಥಿಗಳು ಅರ್ಜಿ ಪ್ರಕ್ರಿಯೆ ಶುಲ್ಕದಿಂದ ವಿನಾಯಿತಿ ಪಡೆದಿರುತ್ತಾರೆ.', 'ನಾನು ಪ್ರೀತಿಸಿದ್ದೇನೆ.', 'ಯಾವಾಗಲೂ ಎಲ್ಲಾ ಪದಾರ್ಥಗಳನ್ನು ಹೊಂದಿಲ್ಲ.', 'ದೇಶಾದ್ಯಂತ ಕರೋನಾ ಸೋಂಕಿತರ ಸಂಖ್ಯೆ 23 ಸಾವಿರಕ್ಕೂ ಹೆಚ್ಚಿದ್ದು, ಒಟ್ಟು ಈವರೆಗೆ 718 ಜನರು ಮೃತಪಟ್ಟಿದ್ದಾರೆ', 'ಅವರ ಜೀವನವನ್ನು ರಾಷ್ಟ್ರ ಸೇವೆಗೆ ಮಾತ್ರ ಮುಡಿಪಾಗಿಟ್ಟಿದ್ದರು ಎಂದು ತಿಳಿಸಿದರು.', 'ಬಾಲ್ಯದಲ್ಲೇ ತಂದೆಯನ್ನು ಕಳೆದುಕೊಂಡರು.', 'ಕಾರು ಸಂಪೂರ್ಣ ಬೆಂಕಿಗೆ ಆಹುತಿ ಆಗಿದೆ.', 'ಆದರೆ ಕೊನೇ ಕ್ಷಣದಲ್ಲಿ ತಮ್ಮ ನಿರ್ಧಾರದಿಂದ ಹಿಂದೆ ಸರಿದಿದ್ದಾರೆ.', 'ಪ್ರವಾಹದಿಂದ ಬಹಳಷ್ಟು ಆಸ್ತಿ ನಷ್ಟವಾಗಿದ್ದು, ಜನರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಮುಂದಿನ ಕೆಲವು ತಿಂಗಳಲ್ಲಿ ಈ ಯೋಜನೆಯನ್ನು ಪ್ರಕಟಿಸಲಾಗುವುದು.', 'ಚೆಂದ ಇಲ್ಲದಿದ್ದರೇನು?', 'ಅದಕ್ಕ ವನು--ನನ್ನ ಬಳಿಗೆ ಹೊಸ ಗಡಿಗೆಯನ್ನು ತಕ್ಕೊಂಡು ಬಂದು ಅದರಲ್ಲಿ ಉಪ್ಪು ಹಾಕಿರಿ ಅಂದನು. ಅವರು ಅದನ್ನು ಅವನ ಬಳಿಗೆ ತಕ್ಕೊಂಡು ಬಂದರು.', 'ಪ್ರಮುಖವಾಗಿಯೂ ಹೋಂಡಾ ಆಕ್ಟಿವಾ 125, ಸುಜುಕಿ ಆಕ್ಸೆಸ್ 125 ಹಾಗೂ ಮಹೀಂದ್ರ ಗಸ್ಟೊ 125 ಮಾದರಿಗಳಿಗೆ ಟಿವಿಎಸ್ ನೂತನ ಸ್ಕೂಟರ್ ಪ್ರತಿಸ್ಪರ್ಧಿಯಾಗಲಿದೆ', '900 ಚದರ ಅಡಿ ಜಾಗದ ಭೂ ದಾಖಲೆಗಳನ್ನು ಅವರು ನಗರ ಪಂಚಾಯತ್ ಅಧ್ಯಕ್ಷ ಜಹೀರ್ ಫಾರೂಕಿ ಅವರಿಗೆ ಹಸ್ತಾಂತರಿಸಿದರು.')]\n",
      "[('the engine is mated to a six-speed gearbox with a slip and assist clutch', 'this is what he has said.', 'but it is not taking off.', \"conflicting reports on amitabh bachchan's health confuse fans\", 'howsoever powerful.', 'the conversation thereafter went as follows:', 'what is the opposition doing?', 'best of luck to all team members.', 'but it is very expensive.', \"shah rukh khan's next film is titled sanki and will be directed by tamil director atlee.\", 'thats exactly what organizations are saying.', 'bike rider killed in motorcycle-tanker collision', 'students need not worry about it, he said.', 'there is no risk to anyone.', 'the matter had created uproar across the state.', 'parents are overjoyed to see the success of their children.', 'elections will be held soon.', 'to and fro vehicular traffic to the hill from uttanahalli road side has been completely banned.', 'do you leave?', 'define your goals and objectives.', \"what's up.\", 'how long will you take for completing the investigation?', 'there are two engine options.', 'the film in question has not yet received the certificate from censor board.', '\"even chhatrapati shivaji maharaj faced opposition from his own family,\"\" he said.\"', 'how to make a cake at home?', 'there was no harm to any passenger due to the fire.', 'they too moved their forces.', 'development has taken a hit in the state.', 'and this can be done by:'), ('ಈ ಎಂಜಿನ್ ಅನ್ನು 6- ಸ್ಪೀಡ್ ಗೇರ್ಬಾಕ್ಸ್ಗೆ ಅಸಿಸ್ಟ್ ಮತ್ತು ಸ್ಲಿಪ್ಪರ್ ಕ್ಲಚ್ನೊಂದಿಗೆ ಜೋಡಿಸಲಾಗಿದೆ', 'ಈ ಮೂಲಕ ತಾವೇನೆಂಬುದನ್ನು ತಿಳಿಸಿದ್ದಾರೆ.', 'ಆದರೆ, ಅದನ್ನು ಬಿಡುಗಡೆ ಮಾಡುತ್ತಿಲ್ಲ.', 'ಅಭಿಮಾನಿಗಳಲ್ಲಿ ಗೊಂದಲ ಸೃಷ್ಟಿಸಿದ ಅಮಿತಾಬ್ ಬಚ್ಚನ್ ಅನಾರೋಗ್ಯ ಸುದ್ದಿ', 'ಷ್ಟೇ ಬಲಶಾಲಿ ಆಗಿರಲಿ.', 'ಆನಂತರ ನಡೆದ ಸಂಭಾಷಣೆಯ ಸಾರಾಂಶ ಹೀಗಿದೆ.', 'ವಿರೋಧ ಪಕ್ಷದವರ ಕೆಲಸ ಏನಿದೆ?', 'ತಂಡದ ಸದಸ್ಯರಿಗೆಲ್ಲಾ ಅದೃಷ್ಟ ತಂದುಕೊಡುತ್ತವೆ.', 'ಆದರೆ ಆರ್ಥಿಕ ದೃಷ್ಟಿಯಿಂದ ಬಹಳ ದುಬಾರಿಯಾಗಿರುತ್ತದೆ.', 'ಶಾರುಖ್ ಖಾನ್ ಅವರ ಮುಂದಿನ ಚಲನಚಿತ್ರವನ್ನು ತಮಿಳು ನಿರ್ದೇಶಕ ಅಟ್ಲೀ ನಿರ್ದೇಶನ ಮಾಡಲಿದ್ದಾರೆ.', 'ಹಾಗೆಂದು ಆ ಸಂಸ್ಥೆಯೇ ಹೇಳಿಕೊಳ್ಳುತ್ತಿದೆ.', 'ಕಬ್ಬು ಸಾಗಿಸುತ್ತಿದ್ದ ಲಾರಿ ಹಾಗೂ ಬೈಕ್ ನಡುವೆ ಡಿಕ್ಕಿ : ಸವಾರ ಸಾವು', 'ಇದರ ಬಗ್ಗೆ ಯಾವುದೇ ಆತಂಕ ಬೇಡ ಎಂದು ವಿದ್ಯಾರ್ಥಿಗಳಿಗೆ ತಿಳಿಸಿದರು.', 'ಯಾರಿಗೂ ಅಂತಹ ಅಪಾಯಗಳಾಗಿಲ್ಲ ಎಂದರು.', 'ಈ ಸುದ್ದಿ ರಾಜ್ಯಾದ್ಯಂತ ಚರ್ಚೆಗೆ ಕಾರಣವಾಗಿತ್ತು.', 'ಪ್ರತಿ ಹೆತ್ತವರಿಗೂ ಅವರವರ ಮಕ್ಕಳ ಸಾಧನೆ ನೋಡೋ ಖುಷಿ ಇದ್ದೇ ಇರುತ್ತೆ.', 'ಚುನಾವಣೆ ಇನ್ನೇನು ಸಧ್ಯದಲ್ಲೇ ನಡೆಯಲಿದೆ.', 'ಉತ್ತನಹಳ್ಳಿ ರಸ್ತೆಯ ಕಡೆಯಿಂದ ಚಾಮುಂಡಿ ಬೆಟ್ಟಕ್ಕೆ ಬರುವ ಮತ್ತು ಹೋಗುವ ವಾಹನಗಳಿಗೆ ಸಂಚಾರವನ್ನು ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಬಂಧಿಸಲಾಗಿದೆ.', 'ಬಿಡಲಿಕ್ಕೆ ಆಗುತ್ತಾ?', 'ನಿಮ್ಮ ಗುರಿ ಮತ್ತು ಉದ್ದೇಶಗಳನ್ನು ಗುರುತಿಸಿ.', 'ಏನು ಉರುಳೇ.', 'ತನಿಖೆಯನ್ನು ಪೂರ್ಣಗೊಳಿಸಲು ನಿಮಗೆ ಇನ್ನೂ ಎಷ್ಟು ಸಮಯ ಬೇಕು?', 'ಎರಡು ಎಂಜಿನ್ ಆಯ್ಕೆಗಳಿವೆ.', 'ಸೆನ್ಸಾರ್ ಮಂಡಳಿಯಿಂದ ಈ ಸಿನಿಮಾಗೆ ಇನ್ನೂ ಪ್ರಮಾಣಪತ್ರ ನೀಡಿಲ್ಲ.', 'ಛತ್ರಪತಿ ಶಿವಾಜಿ ಕೂಡ ತಮ್ಮದೇ ಕುಟುಂಬದಿಂದಲೇ ವಿರೋಧವನ್ನು ಎದುರಿಸಿದ್ದರು.', 'ಹೇಗೆ ಮನೆಯಲ್ಲಿ ಒಂದು ಕಾರ್ಟೂನ್ ರಚಿಸಲು?', 'ಬೆಂಕಿ ಅವಘಡದಲ್ಲಿ ಪ್ರಯಾಣಿಕರ ಪ್ರಾಣಕ್ಕೆ ಹಾನಿಯಾಗಿಲ್ಲ.', 'ತಮ್ಮ ಅಧಿಕಾರವನ್ನು ಚಲಾಯಿಸಿದ್ದರು ಕೂಡಾ.', 'ರಾಜ್ಯದಲ್ಲಿ ಅಭಿವೃದ್ಧಿ ಪರ್ವ ಬಂದಿದೆ.', 'ಮತ್ತು ನೀವು ಇದನ್ನು ಪರಿಹರಿಸಬಹುದು:')]\n",
      "[('a proposal has been made.', 'but the government is unmoved.', 'the programme was attended by students and teachers of the school.', 'they taught me much about the true holy father in heaven, jehovah god.', 'what we can do', 'the bjp is trying to gain a strong foothold in the state.', 'farmers are in distress all over the country.', 'but euphoria, relief, and achievement likewise provoke emotional tears in this case, tears of joy.', 'and to pass by you into macedonia, and to come again out of macedonia unto you, and of you to be brought on my way toward judaea.', 'in case of an attack, we need to respond swiftly to minimise the damage.', 'for unknown reasons.', 'community spread', 'which board?', 'there are 14 girls and 12 junior players.', 'preparing for the role', 'recipe: banana blossom salad', 'do not isolate yourself from your faithful christian brothers and sisters.', 'patient with leukaemia disease.', 'the total length of the highway road is 250 km.', 'the governors post is a constitutional post.', 'development and growth', 'it was even found among the ones he had chosen as apostles!', 'in their hands lies the future of india.', 'government high school', 'they stay in their own world.', 'you cant be regretful.', 'location: ranga rao road, near shankar mutt, shankarapura, near basavanagudi, bangalore', \"that's all humbug.\", 'they were all there.', 'he was keen to learn english also.'), ('ಪ್ರಸ್ತಾವನೆ ಸಲ್ಲಿಸಿತ್ತು.', 'ಆದರೆ ಈ ಸರ್ಕಾರ ಸದ್ಯ ಅತಂತ್ರದಲ್ಲಿದೆ.', 'ಈ ಕಾರ್ಯಕ್ರಮದಲ್ಲಿ ಶಿಕ್ಷಕ ವೃಂದ ಹಾಗೂ ವಿದ್ಯಾರ್ಥಿಗಳು ಭಾಗಿಯಾಗಿದ್ದರು.', 'ಅವರು ನನಗೆ ಸ್ವರ್ಗದಲ್ಲಿರುವ ನಿಜವಾದ ಪವಿತ್ರ ತಂದೆ ಯೆಹೋವ ದೇವರ ಬಗ್ಗೆ ಬಹಳಷ್ಟನ್ನು ಕಲಿಸಿದರು.', 'ನಾವೇನು ಮಾಡಬಹುದು ?', 'ರಾಜ್ಯದಲ್ಲಿ ಬಿಜೆಪಿ ಅಧಿಕಾರ ಪಡೆದುಕೊಳ್ಳಲು ಶಥಾಯ ಗಥಾಯ ಪ್ರಯತ್ನ ಮಾಡುತ್ತಿದೆ', 'ದೇಶಾದ್ಯಂತ ರೈತರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಭಾವನಾತ್ಮಕ ಕಣ್ಣೀರು ಉಕ್ಕಿ ಬರಲು ಅನೇಕ ಕಾರಣಗಳಿವೆ.', 'ತರುವಾಯ ನಿಮ್ಮ ಮಾರ್ಗವಾಗಿ ಮಕೆದೋನ್ಯಕ್ಕೆ ಹೋಗಿ ತಿರಿಗಿ ಮಕೆದೋನ್ಯದಿಂದ ನಿಮ್ಮ ಬಳಿಗೆ ಬಂದು ನಿಮ್ಮಿಂದ ಯೂದಾಯಕ್ಕೆ ಸಾಗಕಳುಹಿಸಲ್ಪಡುವಂತೆಯೂ ಯೋಚಿಸಿದ್ದೆನು.', 'ದಾಳಿಯ ಸಂದರ್ಭದಲ್ಲಿ ಹಾನಿಯನ್ನು ಕನಿಷ್ಠಗೊಳಿಸಲು ನಾವು ಚುರುಕಾಗಿ ಕಾರ್ಯಾಚರಿಸಬೇಕಾಗುತ್ತದೆ.', 'ಸ್ಪಷ್ಟೀಕರಿಸದ ಕಾರಣಗಳಿಗಾಗಿ', 'ಸಮುದಾಯ ಹರಡುವಿಕೆ', 'ಯಾರಿಗೆ ಯಾವ ಮಂಡಳಿ?', 'ಇವುಗಳಲ್ಲಿ 14 ಗಂಡು, 12 ಹೆಣ್ಣು ಚಿರತೆ ಸೇರಿವೆ.', 'ಪಾತ್ರಕ್ಕಾಗಿ ತಯಾರಿ', '\"ಪಾಕವಿಧಾನ: ಹೂಕೋಸು \"\"ಆಲೂಗಡ್ಡೆ\"\" ಸಲಾಡ್\"', 'ನಂಬಿಗಸ್ತ ಸಹೋದರ ಸಹೋದರಿಯರಿಂದ ನಿಮ್ಮನ್ನು ಪ್ರತ್ಯೇಕಿಸಿಕೊಳ್ಳಬೇಡಿ.', 'ತೀವ್ರತರವಾದ ಲ್ಯುಕೇಮಿಯಾ ಬಳಲುತ್ತಿರುವ ರೋಗಿಗಳು.', 'ರಸ್ತೆಯಲ್ಲಿ ಒಟ್ಟು ಅಗಲ - 250 ಮೀಟರ್.', 'ರಾಜ್ಯಪಾಲರದು ಸಂವಿಧಾನಾತ್ಮಕ ಹುದ್ದೆ.', 'ಅಭಿವೃದ್ಧಿ ಮತ್ತು ಅನಾರೋಗ್ಯ', 'ಯೇಸು ಸಾಯುವ ದಿನದ ವರೆಗೂ ಅವರು ಹೆಬ್ಬಯಕೆಯನ್ನು ತೋರ್ಪಡಿಸಿದರು.', 'ಇವರ ಆಟದ ಮೇಲೆ ಭಾರತದ ಭವಿಷ್ಯ ನಿಂತಿದೆ.', 'ಸಮಸ್ಯೆಗಳ ಆಗರ ಸರ್ಕಾರಿ ಪ್ರೌಢಶಾಲೆ', 'ಇವರು ತಮ್ಮದೇ ಆಗಿರುವ ಲೋಕದಲ್ಲಿ ವಿಹರಿಸುತ್ತಾ ಇರುವರು.', 'ನಿಮಗೆ ಕ್ಷಮಿಸುವ ಮನಸ್ಸಿಲ್ಲದಿರಬಹುದು.', 'ಸ್ಥಳ : ಜಕ್ಕಸಂದ್ರ, ಸರ್ಜಾಪುರ ರಸ್ತೆ, ಬೆಂಗಳೂರು', 'ಅದೆಲ್ಲ ಗಿಮಿಕ್ ಅಷ್ಟೆ.', 'ಅವರೆಲ್ಲರೂ ಇದ್ದರು.', 'ಅವರಿಗೆ ಇಂಗ್ಲಿಷ್ ಕಲಿಯುವುದಕ್ಕೆ ತುಂಬಾ ಆಸೆ ಇತ್ತು.')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57f5d861-7b3d-41cf-aa69-69feee98d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN], reduction='none')\n",
    "\n",
    "# when computing the loss, we are ignoring cases when the lable is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f4a304d-e484-460a-996d-b4fd8c684a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, kn_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
    "      eng_chars_to_padding_mask = torch.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "      kn_chars_to_padding_mask = torch.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68242173-3607-4107-bccf-0ebd8236117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english to index size: 70\n",
      "kn vocabulary size: 125\n"
     ]
    }
   ],
   "source": [
    "'''print(\"english to index size:\", len(english_to_index))\n",
    "print(\"kn vocabulary size:\", kn_vocab_size)\n",
    "\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, kn_batch, english_to_index, kn_vocab_size):\n",
    "  \"\"\"\n",
    "  Creates attention masks for encoder-decoder model.\n",
    "\n",
    "  Args:\n",
    "      eng_batch: List of English token sequences (torch.Tensor).\n",
    "      kn_batch: List of Kannada token sequences (torch.Tensor).\n",
    "      english_to_index: Dictionary mapping English words to their index.\n",
    "      kn_vocab_size: Size of the Kannada vocabulary.\n",
    "\n",
    "  Returns:\n",
    "      encoder_self_attention_mask: Mask for encoder self-attention (torch.Tensor).\n",
    "      decoder_self_attention_mask: Mask for decoder self-attention (torch.Tensor).\n",
    "      decoder_cross_attention_mask: Mask for decoder-encoder cross-attention (torch.Tensor).\n",
    "  \"\"\"\n",
    "\n",
    "  num_sentences = len(eng_batch)\n",
    "  look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)\n",
    "  look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "\n",
    "  # Check for padding token existence and index\n",
    "  if PADDING_TOKEN not in english_to_index or PADDING_TOKEN >= kn_vocab_size:\n",
    "      raise ValueError(\"Padding token index (PADDING_TOKEN) is out of range. Please check vocabulary size.\")\n",
    "\n",
    "  pad_idx = english_to_index.get(PADDING_TOKEN)\n",
    "\n",
    "\n",
    "  encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "  decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "  decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)\n",
    "\n",
    "  for idx in range(num_sentences):\n",
    "    eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
    "    eng_chars_to_padding_mask = torch.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "    kn_chars_to_padding_mask = torch.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "\n",
    "    encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "    encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "    decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "    decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "    decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "    decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "  encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "  decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "  decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "\n",
    "  return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e3b88-3303-4cbd-a6b8-f6ad4a8a4efa",
   "metadata": {},
   "source": [
    "Modify mask such that the padding tokens cannot look ahead. In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\r\n",
    "\r\n",
    "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "449d6cd2-e704-4698-89d2-c0c00cdb92dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 5.609839916229248\n",
      "English: hes a scientist.\n",
      "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
      "Kannada Prediction: ಷಕಕಕಷಕಷಕಷಷಕಷಷಷಷಷಌಷಋಷಋಷಋಋಮಮಮಮಮ೫೫ಉಒಮ\"ಒಮಮ999೫9ಮಕಮಮ಼\"ಅಷ೨೭಼ಮ1ಎಬಮಮ೨ಎಎಒಸಷಗಒಬ೨೪ಹఇಏಏಏಏಏಏಏಏಏಏಱಏಔಏಔಷಷಷಮಮಮೆೆೆಮ಼ಮಮಮఇೆಏ಼ೆఆಮಒ೫೫ఆఆఆ\"ఆಮఆಮಮ\"ಮಮಮಮಮಮ6ಏ\n",
      "Evaluation translation (should we go to the mall?) : ('      ಕಕಕ                  ಳ                                                           ೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆ    ೆೆೆೆೆೆೆ  ೆೆೆೆ           ೆೆೆೆ           ೆೆೆೆ     ೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆೆ          ೆೆೆೆೆ',)\n",
      "-------------------------------------------\n",
      "Iteration 100 : 3.5358498096466064\n",
      "English: she ate it.\n",
      "Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.\n",
      "Kannada Prediction: ್ದ ರ     ್್   ್್ಿ\n",
      "Evaluation translation (should we go to the mall?) : ('ನರರ                 ಿ   ಿಿಿಿಿendas',)\n",
      "-------------------------------------------\n",
      "Iteration 200 : 3.2440783977508545\n",
      "English: caste and religion were unknown.\n",
      "Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.\n",
      "Kannada Prediction: ಇಈದ್  ನ  ್ನ್     ಲಿ ಿ ್ ಿಿ ಿಲ \n",
      "Evaluation translation (should we go to the mall?) : ('ಇದರ್   ನ ಸಿ ಸಿ ಸಿ ನಿ ನಿಲಿ ನಿಲಿ ನಿ ಲಿದಿದಿ ಲಿಲಿದಿದendas',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'`'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, kn_batch)\n\u001b[0;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m kn_predictions \u001b[38;5;241m=\u001b[39m transformer(eng_batch,\n\u001b[0;32m     15\u001b[0m                              kn_batch,\n\u001b[0;32m     16\u001b[0m                              encoder_self_attention_mask\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     17\u001b[0m                              decoder_self_attention_mask\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     18\u001b[0m                              decoder_cross_attention_mask\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     19\u001b[0m                              enc_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m                              enc_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m                              dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m                              dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(kn_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[0;32m     25\u001b[0m     kn_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kn_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:270\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, encoder_self_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[0;32m    263\u001b[0m             decoder_self_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    264\u001b[0m             decoder_cross_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m             dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# we should make this true \u001b[39;00m\n\u001b[0;32m    268\u001b[0m             dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m): \u001b[38;5;66;03m# we should make this true\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, encoder_self_attention_mask, start_token\u001b[38;5;241m=\u001b[39menc_start_token, end_token\u001b[38;5;241m=\u001b[39menc_end_token)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token\u001b[38;5;241m=\u001b[39mdec_start_token, end_token\u001b[38;5;241m=\u001b[39mdec_end_token)\n\u001b[0;32m    272\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:167\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, self_attention_mask, start_token, end_token)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_attention_mask, start_token, end_token):\n\u001b[1;32m--> 167\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embedding(x, start_token, end_token)\n\u001b[0;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, self_attention_mask)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:69\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_tokenize(x, start_token, end_token)\n\u001b[0;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     71\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:64\u001b[0m, in \u001b[0;36mSentenceEmbedding.batch_tokenize\u001b[1;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[0;32m     62\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[1;32m---> 64\u001b[0m    tokenized\u001b[38;5;241m.\u001b[39mappend( tokenize(batch[sentence_num], start_token, end_token) )\n\u001b[0;32m     65\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(tokenized)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39mto(get_device())\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:53\u001b[0m, in \u001b[0;36mSentenceEmbedding.batch_tokenize.<locals>.tokenize\u001b[1;34m(sentence, start_token, end_token)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(sentence, start_token, end_token):\n\u001b[1;32m---> 53\u001b[0m     sentence_word_indicies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_to_index[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sentence)]\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_token:\n\u001b[0;32m     55\u001b[0m         sentence_word_indicies\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_to_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART_TOKEN])\n",
      "File \u001b[1;32m~\\Transformers\\Transformer.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(sentence, start_token, end_token):\n\u001b[1;32m---> 53\u001b[0m     sentence_word_indicies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_to_index[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(sentence)]\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_token:\n\u001b[0;32m     55\u001b[0m         sentence_word_indicies\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_to_index[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART_TOKEN])\n",
      "\u001b[1;31mKeyError\u001b[0m: '`'"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, kn_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
    "        optim.zero_grad()\n",
    "        kn_predictions = transformer(eng_batch,\n",
    "                                     kn_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start_token=False,\n",
    "                                     enc_end_token=False,\n",
    "                                     dec_start_token=True,\n",
    "                                     dec_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(\n",
    "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
    "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in kn_sentence_predicted:\n",
    "              if idx == kannada_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += index_to_kannada[idx.item()]\n",
    "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            kn_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_sequence_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          kn_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          enc_start_token=False,\n",
    "                                          enc_end_token=False,\n",
    "                                          dec_start_token=True,\n",
    "                                          dec_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_kannada[next_token_index]\n",
    "                kn_sentence = (kn_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
    "            print(\"-------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6eff6-54dc-4d10-9bdc-1ee05766e44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
